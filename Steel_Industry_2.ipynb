{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFCazklRDonM",
        "outputId": "9ebb5855-22e2-483b-db3e-eb078fd79f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Steel Industry Energy Consumption Dataset...\n",
            "Dataset shape: (35040, 11)\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35040 entries, 0 to 35039\n",
            "Data columns (total 11 columns):\n",
            " #   Column                                Non-Null Count  Dtype  \n",
            "---  ------                                --------------  -----  \n",
            " 0   date                                  35040 non-null  object \n",
            " 1   Usage_kWh                             35040 non-null  float64\n",
            " 2   Lagging_Current_Reactive.Power_kVarh  35040 non-null  float64\n",
            " 3   Leading_Current_Reactive_Power_kVarh  35040 non-null  float64\n",
            " 4   CO2(tCO2)                             35040 non-null  float64\n",
            " 5   Lagging_Current_Power_Factor          35040 non-null  float64\n",
            " 6   Leading_Current_Power_Factor          35040 non-null  float64\n",
            " 7   NSM                                   35040 non-null  int64  \n",
            " 8   WeekStatus                            35040 non-null  object \n",
            " 9   Day_of_week                           35040 non-null  object \n",
            " 10  Load_Type                             35040 non-null  object \n",
            "dtypes: float64(6), int64(1), object(4)\n",
            "memory usage: 2.9+ MB\n",
            "None\n",
            "\n",
            "First few rows:\n",
            "               date  Usage_kWh  Lagging_Current_Reactive.Power_kVarh  \\\n",
            "0  01/01/2018 00:15       3.17                                  2.95   \n",
            "1  01/01/2018 00:30       4.00                                  4.46   \n",
            "2  01/01/2018 00:45       3.24                                  3.28   \n",
            "3  01/01/2018 01:00       3.31                                  3.56   \n",
            "4  01/01/2018 01:15       3.82                                  4.50   \n",
            "\n",
            "   Leading_Current_Reactive_Power_kVarh  CO2(tCO2)  \\\n",
            "0                                   0.0        0.0   \n",
            "1                                   0.0        0.0   \n",
            "2                                   0.0        0.0   \n",
            "3                                   0.0        0.0   \n",
            "4                                   0.0        0.0   \n",
            "\n",
            "   Lagging_Current_Power_Factor  Leading_Current_Power_Factor   NSM  \\\n",
            "0                         73.21                         100.0   900   \n",
            "1                         66.77                         100.0  1800   \n",
            "2                         70.28                         100.0  2700   \n",
            "3                         68.09                         100.0  3600   \n",
            "4                         64.72                         100.0  4500   \n",
            "\n",
            "  WeekStatus Day_of_week   Load_Type  \n",
            "0    Weekday      Monday  Light_Load  \n",
            "1    Weekday      Monday  Light_Load  \n",
            "2    Weekday      Monday  Light_Load  \n",
            "3    Weekday      Monday  Light_Load  \n",
            "4    Weekday      Monday  Light_Load  \n",
            "\n",
            "==================================================\n",
            "STEP 1: MISSING VALUES ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Missing values count:\n",
            "Series([], dtype: int64)\n",
            "No missing values found!\n",
            "\n",
            "==================================================\n",
            "STEP 2: DATA DISTRIBUTION ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Target variable (Usage_kWh) statistics:\n",
            "count    35040.000000\n",
            "mean        27.386892\n",
            "std         33.444380\n",
            "min          0.000000\n",
            "25%          3.200000\n",
            "50%          4.570000\n",
            "75%         51.237500\n",
            "max        157.180000\n",
            "Name: Usage_kWh, dtype: float64\n",
            "\n",
            "date distribution:\n",
            "date\n",
            "31/12/2018 20:00    1\n",
            "31/12/2018 19:45    1\n",
            "31/12/2018 19:30    1\n",
            "31/12/2018 19:15    1\n",
            "31/12/2018 19:00    1\n",
            "                   ..\n",
            "01/01/2018 01:15    1\n",
            "01/01/2018 01:00    1\n",
            "01/01/2018 00:45    1\n",
            "01/01/2018 00:30    1\n",
            "01/01/2018 00:15    1\n",
            "Name: count, Length: 35040, dtype: int64\n",
            "\n",
            "WeekStatus distribution:\n",
            "WeekStatus\n",
            "Weekday    25056\n",
            "Weekend     9984\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Day_of_week distribution:\n",
            "Day_of_week\n",
            "Monday       5088\n",
            "Tuesday      4992\n",
            "Wednesday    4992\n",
            "Thursday     4992\n",
            "Friday       4992\n",
            "Saturday     4992\n",
            "Sunday       4992\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Load_Type distribution:\n",
            "Load_Type\n",
            "Light_Load      18072\n",
            "Medium_Load      9696\n",
            "Maximum_Load     7272\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Load_Type distribution:\n",
            "Load_Type\n",
            "Light_Load      18072\n",
            "Medium_Load      9696\n",
            "Maximum_Load     7272\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "STEP 3: FEATURE SCALING (Will be applied after feature engineering)\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "STEP 4: OUTLIER DETECTION AND TREATMENT\n",
            "==================================================\n",
            "Usage_kWh: 328 outliers detected\n",
            "Lagging_Current_Reactive.Power_kVarh: 1059 outliers detected\n",
            "Leading_Current_Reactive_Power_kVarh: 7759 outliers detected\n",
            "CO2(tCO2): 437 outliers detected\n",
            "Lagging_Current_Power_Factor: 1 outliers detected\n",
            "Leading_Current_Power_Factor: 8327 outliers detected\n",
            "Outliers treated using IQR method with capping\n",
            "\n",
            "==================================================\n",
            "STEP 5: CATEGORICAL FEATURE ENCODING\n",
            "==================================================\n",
            "Date column removed\n",
            "Categorical columns found: ['WeekStatus', 'Day_of_week', 'Load_Type']\n",
            "Label encoded: Load_Type\n",
            "One-hot encoded: WeekStatus\n",
            "One-hot encoded: Day_of_week\n",
            "Reason: Label encoding for ordinal features preserves order relationships,\n",
            "while one-hot encoding for nominal features avoids imposing false ordinality.\n",
            "\n",
            "==================================================\n",
            "STEP 6: FEATURE ENGINEERING\n",
            "==================================================\n",
            "Creating lag features...\n",
            "Creating rolling window features...\n",
            "Creating interaction features...\n",
            "Feature engineering completed. New shape: (35040, 32)\n",
            "\n",
            "==================================================\n",
            "STEP 7: FEATURE SELECTION\n",
            "==================================================\n",
            "Method 1: Mutual Information for feature selection\n",
            "Reason: Mutual information captures both linear and non-linear relationships\n",
            "between features and target, making it suitable for regression tasks.\n",
            "Selected features using Mutual Information: 15\n",
            "['Lagging_Current_Reactive.Power_kVarh', 'CO2(tCO2)', 'Lagging_Current_Power_Factor', 'NSM', 'Load_Type', 'Lagging_Current_Reactive.Power_kVarh_lag1', 'CO2(tCO2)_lag1', 'Lagging_Current_Power_Factor_lag1', 'Lagging_Current_Reactive.Power_kVarh_rolling_mean3', 'Lagging_Current_Reactive.Power_kVarh_rolling_std3', 'CO2(tCO2)_rolling_mean3', 'CO2(tCO2)_rolling_std3', 'Lagging_Current_Power_Factor_rolling_mean3', 'Total_Reactive_Power', 'Power_Factor_Ratio']\n",
            "\n",
            "Method 2: RFE with Random Forest\n",
            "Reason: RFE with tree-based models can identify feature importance\n",
            "while considering feature interactions.\n",
            "Selected features using RFE: 15\n",
            "['Lagging_Current_Reactive.Power_kVarh', 'Leading_Current_Reactive_Power_kVarh', 'CO2(tCO2)', 'Lagging_Current_Power_Factor', 'NSM', 'Lagging_Current_Reactive.Power_kVarh_lag1', 'CO2(tCO2)_lag1', 'Lagging_Current_Power_Factor_lag1', 'Lagging_Current_Reactive.Power_kVarh_rolling_mean3', 'Lagging_Current_Reactive.Power_kVarh_rolling_std3', 'CO2(tCO2)_rolling_mean3', 'Lagging_Current_Power_Factor_rolling_mean3', 'Lagging_Current_Power_Factor_rolling_std3', 'Total_Reactive_Power', 'Power_Factor_Ratio']\n",
            "\n",
            "Final features after combining both methods: 17\n",
            "\n",
            "==================================================\n",
            "STEP 8: FEATURE TRANSFORMATION\n",
            "==================================================\n",
            "Applied StandardScaler and PowerTransformer (Yeo-Johnson)\n",
            "Reason: StandardScaler ensures all features have equal importance,\n",
            "while PowerTransformer helps achieve normality in feature distributions.\n",
            "\n",
            "==================================================\n",
            "STEP 9: TRAIN-TEST SPLIT\n",
            "==================================================\n",
            "Training set shape: (28032, 17)\n",
            "Test set shape: (7008, 17)\n",
            "\n",
            "==================================================\n",
            "STEP 10: MODEL SELECTION WITH CROSS VALIDATION\n",
            "==================================================\n",
            "Cross-validation results (RMSE):\n",
            "----------------------------------------\n",
            "Linear Regression   : 9.6717 (+/- 0.5967)\n",
            "Ridge Regression    : 9.6742 (+/- 0.6022)\n",
            "Lasso Regression    : 10.4079 (+/- 0.8331)\n",
            "Decision Tree       : 2.5535 (+/- 0.1959)\n",
            "Random Forest       : 1.8680 (+/- 0.0999)\n",
            "Gradient Boosting   : 3.0562 (+/- 0.0475)\n",
            "XGBoost             : 1.8418 (+/- 0.1572)\n",
            "KNN                 : 4.5282 (+/- 0.1971)\n",
            "SVR                 : 5.1955 (+/- 0.5977)\n",
            "\n",
            "Best model: XGBoost (RMSE: 1.8418)\n",
            "\n",
            "==================================================\n",
            "STEP 11: FINAL TRAINING OF BEST MODEL\n",
            "==================================================\n",
            "Best model (XGBoost) trained on full training set\n",
            "\n",
            "==================================================\n",
            "STEP 12: COMPREHENSIVE EVALUATION\n",
            "==================================================\n",
            "\n",
            "Training Set Metrics:\n",
            "MSE:  0.9078\n",
            "RMSE: 0.9528\n",
            "MAE:  0.5313\n",
            "R²:   0.9992\n",
            "\n",
            "Test Set Metrics:\n",
            "MSE:  3.1955\n",
            "RMSE: 1.7876\n",
            "MAE:  0.8163\n",
            "R²:   0.9971\n",
            "\n",
            "Overfitting Analysis:\n",
            "Training R²: 0.9992\n",
            "Test R²:     0.9971\n",
            "Difference:  0.0020\n",
            "✅ Model shows good generalization\n",
            "\n",
            "==================================================\n",
            "STEP 13: HYPERPARAMETER TUNING\n",
            "==================================================\n",
            "Tuning hyperparameters for XGBoost...\n",
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Best CV score: 1.7182\n",
            "\n",
            "==================================================\n",
            "STEP 14: FINAL MODEL VALIDATION\n",
            "==================================================\n",
            "Final Model Performance:\n",
            "\n",
            "Training Set Metrics:\n",
            "MSE:  0.2530\n",
            "RMSE: 0.5030\n",
            "MAE:  0.2749\n",
            "R²:   0.9998\n",
            "\n",
            "Test Set Metrics:\n",
            "MSE:  2.9452\n",
            "RMSE: 1.7162\n",
            "MAE:  0.7096\n",
            "R²:   0.9974\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "                                 feature  importance\n",
            "1                              CO2(tCO2)    0.985774\n",
            "11                  Total_Reactive_Power    0.004861\n",
            "13  Lagging_Current_Reactive.Power_kVarh    0.002933\n",
            "12          Lagging_Current_Power_Factor    0.001434\n",
            "16                        CO2(tCO2)_lag1    0.001123\n",
            "0                     Power_Factor_Ratio    0.000585\n",
            "5                                    NSM    0.000583\n",
            "8   Leading_Current_Reactive_Power_kVarh    0.000571\n",
            "15               CO2(tCO2)_rolling_mean3    0.000533\n",
            "6                              Load_Type    0.000348\n",
            "\n",
            "============================================================\n",
            "FINAL MODEL SUMMARY\n",
            "============================================================\n",
            "Best Model: XGBoost\n",
            "Test RMSE: 1.7162\n",
            "Test R²: 0.9974\n",
            "Number of features: 17\n",
            "\n",
            "✅ Steel Industry Energy Consumption Prediction Model Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Steel Industry Energy Consumption Prediction ML Project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PowerTransformer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from collections import Counter\n",
        "from sklearn.utils import resample\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading Steel Industry Energy Consumption Dataset...\")\n",
        "df = pd.read_csv('Steel_industry_data.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# 1. CHECK MISSING VALUES AND TREAT THEM\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 1: MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\nMissing values count:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "if missing_values.sum() > 0:\n",
        "    # Impute numerical features with median\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    imputer_num = SimpleImputer(strategy='median')\n",
        "    df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Impute categorical features with most frequent\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(cat_cols) > 0:\n",
        "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "        df[cat_cols] = imputer_cat.fit_transform(df[cat_cols])\n",
        "\n",
        "    print(\"Missing values treated successfully!\")\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "\n",
        "# 2. CHECK FOR DATA IMBALANCE (For target variable if categorical)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 2: DATA DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Analyze target variable (Usage_kWh - this appears to be the target for regression)\n",
        "target_col = 'Usage_kWh'\n",
        "print(f\"\\nTarget variable ({target_col}) statistics:\")\n",
        "print(df[target_col].describe())\n",
        "\n",
        "# Check for any categorical columns that might need balancing\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col} distribution:\")\n",
        "    print(df[col].value_counts())\n",
        "\n",
        "# If Load_Type has severe imbalance, we can address it\n",
        "if 'Load_Type' in df.columns:\n",
        "    load_type_counts = df['Load_Type'].value_counts()\n",
        "    print(f\"\\nLoad_Type distribution:\\n{load_type_counts}\")\n",
        "\n",
        "    # Check if balancing is needed (if imbalance ratio > 3:1)\n",
        "    max_count = load_type_counts.max()\n",
        "    min_count = load_type_counts.min()\n",
        "    if max_count / min_count > 3:\n",
        "        print(\"Significant imbalance detected in Load_Type. Applying SMOTE-like balancing...\")\n",
        "        # For regression, we'll use stratified sampling based on Load_Type\n",
        "        # This is more appropriate than upsampling for regression tasks\n",
        "\n",
        "# 3. FEATURE SCALING (Applied later after feature engineering)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 3: FEATURE SCALING (Will be applied after feature engineering)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 4. OUTLIER DETECTION AND TREATMENT\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 4: OUTLIER DETECTION AND TREATMENT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "outlier_info = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "    outlier_info[col] = outliers_count\n",
        "\n",
        "    if outliers_count > 0:\n",
        "        print(f\"{col}: {outliers_count} outliers detected\")\n",
        "        # Cap outliers instead of removing them\n",
        "        df[col] = np.clip(df[col], lower_bound, upper_bound)\n",
        "\n",
        "print(f\"Outliers treated using IQR method with capping\")\n",
        "\n",
        "# 5. ENCODE CATEGORICAL FEATURES\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 5: CATEGORICAL FEATURE ENCODING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Remove date column if present (not useful for modeling)\n",
        "if 'date' in df.columns:\n",
        "    df = df.drop('date', axis=1)\n",
        "    print(\"Date column removed\")\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(f\"Categorical columns found: {list(categorical_cols)}\")\n",
        "\n",
        "# Apply appropriate encoding\n",
        "if len(categorical_cols) > 0:\n",
        "    # For ordinal features like Load_Type, use Label Encoding\n",
        "    # For nominal features, use One-Hot Encoding\n",
        "\n",
        "    ordinal_cols = ['Load_Type']  # Assuming Load_Type might have some order\n",
        "    nominal_cols = [col for col in categorical_cols if col not in ordinal_cols]\n",
        "\n",
        "    # Label Encoding for ordinal\n",
        "    le = LabelEncoder()\n",
        "    for col in ordinal_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "            print(f\"Label encoded: {col}\")\n",
        "\n",
        "    # One-Hot Encoding for nominal\n",
        "    for col in nominal_cols:\n",
        "        if col in df.columns:\n",
        "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "            print(f\"One-hot encoded: {col}\")\n",
        "\n",
        "print(\"Reason: Label encoding for ordinal features preserves order relationships,\")\n",
        "print(\"while one-hot encoding for nominal features avoids imposing false ordinality.\")\n",
        "\n",
        "# 6. FEATURE ENGINEERING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 6: FEATURE ENGINEERING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create time-based features if date information is available in other columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target_col)\n",
        "\n",
        "# Create lag features for time series patterns\n",
        "print(\"Creating lag features...\")\n",
        "for col in numeric_cols[:5]:  # Limit to avoid too many features\n",
        "    df[f'{col}_lag1'] = df[col].shift(1).fillna(df[col].mean())\n",
        "\n",
        "# Create rolling window features\n",
        "print(\"Creating rolling window features...\")\n",
        "for col in numeric_cols[:5]:\n",
        "    df[f'{col}_rolling_mean3'] = df[col].rolling(window=3, min_periods=1).mean()\n",
        "    df[f'{col}_rolling_std3'] = df[col].rolling(window=3, min_periods=1).std().fillna(0)\n",
        "\n",
        "# Create interaction features between important variables\n",
        "print(\"Creating interaction features...\")\n",
        "if 'Lagging_Current_Reactive.Power_kVarh' in df.columns and 'Leading_Current_Reactive_Power_kVarh' in df.columns:\n",
        "    df['Total_Reactive_Power'] = df['Lagging_Current_Reactive.Power_kVarh'] + df['Leading_Current_Reactive_Power_kVarh']\n",
        "\n",
        "# Create ratio features\n",
        "if 'Lagging_Current_Power_Factor' in df.columns and 'Leading_Current_Power_Factor' in df.columns:\n",
        "    df['Power_Factor_Ratio'] = df['Lagging_Current_Power_Factor'] / (df['Leading_Current_Power_Factor'] + 1e-8)\n",
        "\n",
        "print(f\"Feature engineering completed. New shape: {df.shape}\")\n",
        "\n",
        "# 7. FEATURE SELECTION\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 7: FEATURE SELECTION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X = df.drop(target_col, axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "print(\"Method 1: Mutual Information for feature selection\")\n",
        "print(\"Reason: Mutual information captures both linear and non-linear relationships\")\n",
        "print(\"between features and target, making it suitable for regression tasks.\")\n",
        "\n",
        "# Apply mutual information\n",
        "selector_mi = SelectKBest(mutual_info_regression, k=15)  # Select top 15 features\n",
        "X_selected_mi = selector_mi.fit_transform(X, y)\n",
        "selected_features_mi = X.columns[selector_mi.get_support()]\n",
        "\n",
        "print(f\"Selected features using Mutual Information: {len(selected_features_mi)}\")\n",
        "print(list(selected_features_mi))\n",
        "\n",
        "# Method 2: Recursive Feature Elimination with Random Forest\n",
        "print(\"\\nMethod 2: RFE with Random Forest\")\n",
        "print(\"Reason: RFE with tree-based models can identify feature importance\")\n",
        "print(\"while considering feature interactions.\")\n",
        "\n",
        "rf_selector = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfe_selector = RFE(rf_selector, n_features_to_select=15)\n",
        "rfe_selector.fit(X, y)\n",
        "selected_features_rfe = X.columns[rfe_selector.support_]\n",
        "\n",
        "print(f\"Selected features using RFE: {len(selected_features_rfe)}\")\n",
        "print(list(selected_features_rfe))\n",
        "\n",
        "# Combine both selections (union of features)\n",
        "final_features = list(set(selected_features_mi) | set(selected_features_rfe))\n",
        "X_final = X[final_features]\n",
        "\n",
        "print(f\"\\nFinal features after combining both methods: {len(final_features)}\")\n",
        "\n",
        "# 8. FEATURE TRANSFORMATION\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 8: FEATURE TRANSFORMATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Apply scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_final),\n",
        "                       columns=X_final.columns,\n",
        "                       index=X_final.index)\n",
        "\n",
        "# Apply power transformation for normalization\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "X_transformed = pd.DataFrame(pt.fit_transform(X_scaled),\n",
        "                           columns=X_scaled.columns,\n",
        "                           index=X_scaled.index)\n",
        "\n",
        "print(\"Applied StandardScaler and PowerTransformer (Yeo-Johnson)\")\n",
        "print(\"Reason: StandardScaler ensures all features have equal importance,\")\n",
        "print(\"while PowerTransformer helps achieve normality in feature distributions.\")\n",
        "\n",
        "# 9. TRAIN-TEST SPLIT\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 9: TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# 10. MODEL SELECTION WITH CROSS VALIDATION\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 10: MODEL SELECTION WITH CROSS VALIDATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(random_state=42),\n",
        "    'Lasso Regression': Lasso(random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42),\n",
        "    'KNN': KNeighborsRegressor(),\n",
        "    'SVR': SVR()\n",
        "}\n",
        "\n",
        "cv_results = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) if len(np.unique(y)) < 10 else 5\n",
        "\n",
        "print(\"Cross-validation results (RMSE):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        scores = cross_val_score(model, X_train, y_train,\n",
        "                               cv=5, scoring='neg_root_mean_squared_error')\n",
        "        cv_results[name] = -scores.mean()\n",
        "        print(f\"{name:20}: {cv_results[name]:.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name:20}: Error - {str(e)}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = min(cv_results, key=cv_results.get)\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} (RMSE: {cv_results[best_model_name]:.4f})\")\n",
        "\n",
        "# 11. FINAL TRAINING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 11: FINAL TRAINING OF BEST MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "print(f\"Best model ({best_model_name}) trained on full training set\")\n",
        "\n",
        "# 12. EVALUATION\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 12: COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_metrics(y_true, y_pred, set_name):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n{set_name} Set Metrics:\")\n",
        "    print(f\"MSE:  {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE:  {mae:.4f}\")\n",
        "    print(f\"R²:   {r2:.4f}\")\n",
        "\n",
        "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
        "\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test\")\n",
        "\n",
        "# Check for overfitting\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"Training R²: {train_metrics['R2']:.4f}\")\n",
        "print(f\"Test R²:     {test_metrics['R2']:.4f}\")\n",
        "print(f\"Difference:  {train_metrics['R2'] - test_metrics['R2']:.4f}\")\n",
        "\n",
        "if train_metrics['R2'] - test_metrics['R2'] > 0.1:\n",
        "    print(\"⚠️  Model may be overfitting\")\n",
        "else:\n",
        "    print(\"✅ Model shows good generalization\")\n",
        "\n",
        "# 13. HYPERPARAMETER TUNING\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 13: HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define parameter grids for top models\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 6, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "if best_model_name in param_grids:\n",
        "    print(f\"Tuning hyperparameters for {best_model_name}...\")\n",
        "\n",
        "    param_grid = param_grids[best_model_name]\n",
        "\n",
        "    # Use smaller parameter grid for faster execution\n",
        "    if best_model_name == 'Random Forest':\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [None, 20],\n",
        "            'min_samples_split': [2, 5]\n",
        "        }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        best_model, param_grid, cv=5,\n",
        "        scoring='neg_root_mean_squared_error', n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best CV score: {-grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Update best model\n",
        "    best_model_tuned = grid_search.best_estimator_\n",
        "\n",
        "else:\n",
        "    print(f\"No hyperparameter tuning defined for {best_model_name}\")\n",
        "    best_model_tuned = best_model\n",
        "\n",
        "# 14. FINAL MODEL VALIDATION\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STEP 14: FINAL MODEL VALIDATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Final predictions with tuned model\n",
        "y_train_pred_final = best_model_tuned.predict(X_train)\n",
        "y_test_pred_final = best_model_tuned.predict(X_test)\n",
        "\n",
        "print(\"Final Model Performance:\")\n",
        "final_train_metrics = calculate_metrics(y_train, y_train_pred_final, \"Training\")\n",
        "final_test_metrics = calculate_metrics(y_test, y_test_pred_final, \"Test\")\n",
        "\n",
        "# Feature importance (if available)\n",
        "if hasattr(best_model_tuned, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': best_model_tuned.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\nTop 10 Most Important Features:\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "# Final model summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Test RMSE: {final_test_metrics['RMSE']:.4f}\")\n",
        "print(f\"Test R²: {final_test_metrics['R2']:.4f}\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "# Save the model and preprocessors (if needed)\n",
        "import joblib\n",
        "\n",
        "# Save the complete pipeline components\n",
        "model_artifacts = {\n",
        "    'model': best_model_tuned,\n",
        "    'scaler': scaler,\n",
        "    'power_transformer': pt,\n",
        "    'selected_features': final_features,\n",
        "    'feature_selector_mi': selector_mi,\n",
        "    'rfe_selector': rfe_selector\n",
        "}\n",
        "\n",
        "# Uncomment to save\n",
        "# joblib.dump(model_artifacts, 'steel_energy_model_artifacts.pkl')\n",
        "# print(\"\\nModel artifacts saved to 'steel_energy_model_artifacts.pkl'\")\n",
        "\n",
        "print(\"\\n✅ Steel Industry Energy Consumption Prediction Model Complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkZx29TiN2cP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}